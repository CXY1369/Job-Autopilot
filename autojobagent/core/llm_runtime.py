"""
LLM è°ƒç”¨è¿è¡Œæ—¶ï¼ˆV2 æ‹†åˆ†ï¼‰

èŒè´£ï¼š
- ç»Ÿä¸€å¤„ç†æ¨¡åž‹å›žé€€é“¾è·¯
- åˆ†ç±»å¸¸è§é”™è¯¯ï¼ˆé™æµ/èƒ½åŠ›ä¸åŒ¹é…/å…¶ä»–ï¼‰
- è¿”å›žç»“æž„åŒ–ç»“æžœä¾›è°ƒç”¨æ–¹å†³å®šåŽç»­çŠ¶æ€
"""

from __future__ import annotations

import time
from dataclasses import dataclass
from typing import Callable


@dataclass
class LLMCallResult:
    ok: bool
    raw: str = ""
    model: str = ""
    model_index: int = 0
    error_summary: str | None = None
    error_code: str | None = None


def run_chat_with_fallback(
    *,
    client,
    fallback_models: list[str],
    start_model_index: int,
    messages: list[dict],
    temperature: float,
    max_tokens: int,
    top_p: float = 0.8,
    on_log: Callable[[str, str], None] | None = None,
    sleep_seconds: float = 1.0,
) -> LLMCallResult:
    """
    åœ¨å€™é€‰æ¨¡åž‹åˆ—è¡¨ä¸Šæ‰§è¡Œå›žé€€è°ƒç”¨ã€‚
    - é™æµæˆ–èƒ½åŠ›ä¸åŒ¹é…ï¼šå°è¯•åˆ‡æ¢åˆ°ä¸‹ä¸€æ¨¡åž‹
    - å…¶ä»–é”™è¯¯ï¼šç«‹å³å¤±è´¥è¿”å›ž
    """
    model_index = max(0, int(start_model_index))
    if model_index >= len(fallback_models):
        model_index = 0

    def _log(level: str, message: str) -> None:
        if on_log:
            on_log(level, message)

    while model_index < len(fallback_models):
        model = fallback_models[model_index]
        try:
            completion = client.chat.completions.create(
                model=model,
                temperature=temperature,
                top_p=top_p,
                max_tokens=max_tokens,
                messages=messages,
            )
            raw = completion.choices[0].message.content or ""
            return LLMCallResult(
                ok=True,
                raw=raw,
                model=model,
                model_index=model_index,
            )
        except Exception as exc:
            error_str = str(exc)
            error_lower = error_str.lower()
            is_rate_limit = "429" in error_str or "rate_limit" in error_lower
            is_capability_mismatch = any(
                kw in error_lower
                for kw in (
                    "does not support",
                    "unsupported",
                    "multimodal",
                    "vision",
                    "image_url",
                    "invalid model",
                    "model_not_found",
                    "not found",
                )
            )

            if is_rate_limit:
                _log("warn", f"âš ï¸ æ¨¡åž‹ {model} é‡åˆ°é€ŸçŽ‡é™åˆ¶")
                model_index += 1
                if model_index < len(fallback_models):
                    _log("info", f"ðŸ”„ åˆ‡æ¢åˆ°æ¨¡åž‹: {fallback_models[model_index]}")
                    time.sleep(max(0.0, sleep_seconds))
                    continue
                return LLMCallResult(
                    ok=False,
                    model=model,
                    model_index=model_index,
                    error_summary="æ‰€æœ‰æ¨¡åž‹éƒ½é‡åˆ°é€ŸçŽ‡é™åˆ¶",
                    error_code="rate_limit_exhausted",
                )

            if is_capability_mismatch:
                _log("warn", f"âš ï¸ æ¨¡åž‹ {model} èƒ½åŠ›ä¸åŒ¹é…æˆ–ä¸å¯ç”¨ï¼Œå°è¯•å›žé€€")
                model_index += 1
                if model_index < len(fallback_models):
                    _log("info", f"ðŸ”„ åˆ‡æ¢åˆ°æ¨¡åž‹: {fallback_models[model_index]}")
                    time.sleep(max(0.0, sleep_seconds))
                    continue
                return LLMCallResult(
                    ok=False,
                    model=model,
                    model_index=model_index,
                    error_summary="æ‰€æœ‰å€™é€‰æ¨¡åž‹éƒ½ä¸æ”¯æŒå½“å‰è¯·æ±‚",
                    error_code="model_unsupported_exhausted",
                )

            return LLMCallResult(
                ok=False,
                model=model,
                model_index=model_index,
                error_summary=f"LLM è°ƒç”¨å¤±è´¥: {exc}",
                error_code="llm_call_failed",
            )

    return LLMCallResult(
        ok=False,
        model=fallback_models[-1] if fallback_models else "",
        model_index=max(0, len(fallback_models) - 1),
        error_summary="LLM æœªè¿”å›žç»“æžœ",
        error_code="llm_no_result",
    )
